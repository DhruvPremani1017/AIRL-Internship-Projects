{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TEXT-DRIVEN IMAGE SEGMENTATION WITH SAM 2 - CLEAN VERSION\n",
        "# ============================================================================\n",
        "# This notebook implements a high-accuracy text-driven image segmentation\n",
        "# pipeline using SAM 2 with enhanced CLIP scoring and comprehensive evaluation\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ Starting Text-Driven Segmentation Pipeline\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1. INSTALLATION & SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q opencv-python-headless scipy scikit-image\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\n",
        "!pip install -q timm einops\n",
        "!pip install -q open_clip_torch\n",
        "!pip install -q matplotlib pillow numpy\n",
        "\n",
        "# Mount Google Drive and create output directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "output_dir = \"/content/drive/MyDrive/Internship_oct/Q2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 2. IMPORTS AND DEVICE SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# CLIP for text-image similarity\n",
        "import open_clip\n",
        "\n",
        "# SAM 2 imports\n",
        "if not os.path.exists('segment-anything-2'):\n",
        "    !git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "sys.path.append('./segment-anything-2')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üîß Device: {device}\")\n",
        "print(f\"üîß PyTorch: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üîß GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 3. HIGH-ACCURACY CLIP SCORER (ENSEMBLE METHOD)\n",
        "# ============================================================================\n",
        "\n",
        "class HighAccuracyCLIPScorer:\n",
        "    \"\"\"Ensemble CLIP scorer for maximum accuracy\"\"\"\n",
        "    \n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "        self.models = {}\n",
        "        self.load_models()\n",
        "    \n",
        "    def load_models(self):\n",
        "        \"\"\"Load best CLIP models for ensemble\"\"\"\n",
        "        print(\"üì• Loading high-accuracy CLIP models...\")\n",
        "        \n",
        "        # Best models for accuracy (tested combinations) - ENHANCED\n",
        "        model_configs = [\n",
        "            ('ViT-L-14', 'openai'),           # OpenAI's best\n",
        "            ('ViT-H-14', 'laion2b_s32b_b79k'), # High-resolution - HIGHEST WEIGHT\n",
        "            ('EVA02-L-14', 'merged2b_s4b_b131k'), # EVA-CLIP\n",
        "            ('ViT-B-16-SigLIP', 'webli'),     # SigLIP for robustness\n",
        "        ]\n",
        "        \n",
        "        for model_name, pretrained in model_configs:\n",
        "            try:\n",
        "                model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "                    model_name, pretrained=pretrained, device=self.device\n",
        "                )\n",
        "                tokenizer = open_clip.get_tokenizer(model_name)\n",
        "                model.eval()\n",
        "                \n",
        "                self.models[model_name] = {\n",
        "                    'model': model,\n",
        "                    'preprocess': preprocess,\n",
        "                    'tokenizer': tokenizer\n",
        "                }\n",
        "                print(f\"  ‚úÖ {model_name} loaded\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå {model_name} failed: {e}\")\n",
        "        \n",
        "        if not self.models:\n",
        "            raise RuntimeError(\"No CLIP models loaded!\")\n",
        "    \n",
        "    def score(self, image, text, use_ensemble=True):\n",
        "        \"\"\"Score image-text similarity with ensemble\"\"\"\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image.astype(np.uint8))\n",
        "        \n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        \n",
        "        scores = []\n",
        "        # ENHANCED WEIGHTS - Based on your original high-accuracy setup\n",
        "        weights = [1.0, 0.9, 0.8, 0.7]  # ViT-H-14 gets highest weight\n",
        "        \n",
        "        for i, (model_name, model_data) in enumerate(self.models.items()):\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    image_input = model_data['preprocess'](image).unsqueeze(0).to(self.device)\n",
        "                    text_input = model_data['tokenizer'](text).to(self.device)\n",
        "                    \n",
        "                    image_features = model_data['model'].encode_image(image_input)\n",
        "                    text_features = model_data['model'].encode_text(text_input)\n",
        "                    \n",
        "                    image_features = F.normalize(image_features, dim=-1)\n",
        "                    text_features = F.normalize(text_features, dim=-1)\n",
        "                    \n",
        "                    similarity = (image_features @ text_features.T).squeeze()\n",
        "                    scores.append(similarity.cpu().item())\n",
        "            except:\n",
        "                scores.append(0.0)\n",
        "        \n",
        "        if not scores:\n",
        "            return 0.0\n",
        "        \n",
        "        # Weighted ensemble\n",
        "        if use_ensemble and len(scores) > 1:\n",
        "            return np.average(scores, weights=weights[:len(scores)])\n",
        "        else:\n",
        "            return scores[0]\n",
        "\n",
        "# Initialize CLIP scorer\n",
        "clip_scorer = HighAccuracyCLIPScorer(device=device)\n",
        "print(\"‚úÖ High-accuracy CLIP scorer ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 4. SAM 2 SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Download SAM 2 checkpoint\n",
        "os.makedirs('./checkpoints', exist_ok=True)\n",
        "sam2_checkpoint = \"./checkpoints/sam2_hiera_base_plus.pt\"\n",
        "model_cfg = \"sam2_hiera_b+.yaml\"\n",
        "\n",
        "if not os.path.exists(sam2_checkpoint):\n",
        "    print(\"üì• Downloading SAM 2 checkpoint...\")\n",
        "    url = \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\"\n",
        "    urllib.request.urlretrieve(url, sam2_checkpoint)\n",
        "    print(\"‚úÖ SAM 2 checkpoint downloaded!\")\n",
        "\n",
        "# Import and setup SAM 2\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "print(\"üîß Building SAM 2 predictor...\")\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
        "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
        "print(\"‚úÖ SAM 2 ready for inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 5. ENHANCED TEXT-TO-REGION DETECTOR (CLIP + GroundingDINO)\n",
        "# ============================================================================\n",
        "\n",
        "# First, let's try to add GroundingDINO for better detection\n",
        "class GroundingDINODetector:\n",
        "    \"\"\"GroundingDINO for high-accuracy text-to-region detection\"\"\"\n",
        "    \n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.load_model()\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Load GroundingDINO model\"\"\"\n",
        "        try:\n",
        "            print(\"üì• Loading GroundingDINO for higher accuracy...\")\n",
        "            # Try to install and load GroundingDINO\n",
        "            import subprocess\n",
        "            import sys\n",
        "            \n",
        "            # Install GroundingDINO\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"groundingdino-py\"])\n",
        "            \n",
        "            from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
        "            from groundingdino.util.inference import Model\n",
        "            \n",
        "            # Download model\n",
        "            model_config = \"GroundingDINO_SwinT_OGC.py\"\n",
        "            model_checkpoint = \"./checkpoints/groundingdino_swint_ogc.pth\"\n",
        "            \n",
        "            if not os.path.exists(model_checkpoint):\n",
        "                print(\"üì• Downloading GroundingDINO checkpoint...\")\n",
        "                url = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\"\n",
        "                urllib.request.urlretrieve(url, model_checkpoint)\n",
        "            \n",
        "            self.model = load_model(model_config, model_checkpoint)\n",
        "            print(\"‚úÖ GroundingDINO loaded successfully!\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  GroundingDINO failed to load: {e}\")\n",
        "            print(\"   Falling back to CLIP-based detection\")\n",
        "            self.model = None\n",
        "    \n",
        "    def detect_objects(self, image, text_prompt):\n",
        "        \"\"\"Detect objects using GroundingDINO\"\"\"\n",
        "        if self.model is None:\n",
        "            return np.array([]), np.array([])\n",
        "        \n",
        "        try:\n",
        "            if isinstance(image, np.ndarray):\n",
        "                image_pil = Image.fromarray(image.astype(np.uint8))\n",
        "            else:\n",
        "                image_pil = image\n",
        "            \n",
        "            # Save temporary image\n",
        "            temp_path = \"/tmp/temp_image.jpg\"\n",
        "            image_pil.save(temp_path)\n",
        "            \n",
        "            # Run GroundingDINO detection\n",
        "            boxes, logits, phrases = predict(\n",
        "                model=self.model,\n",
        "                image=temp_path,\n",
        "                caption=text_prompt,\n",
        "                box_threshold=0.3,\n",
        "                text_threshold=0.25\n",
        "            )\n",
        "            \n",
        "            if len(boxes) > 0:\n",
        "                # Convert to our format\n",
        "                boxes = boxes.cpu().numpy()\n",
        "                scores = torch.softmax(torch.tensor(logits), dim=0).cpu().numpy()\n",
        "                return boxes, scores\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  GroundingDINO detection failed: {e}\")\n",
        "        \n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "class CLIPBasedDetector:\n",
        "    \"\"\"CLIP-based text-to-region detection (most reliable method)\"\"\"\n",
        "    \n",
        "    def __init__(self, clip_scorer):\n",
        "        self.clip_scorer = clip_scorer\n",
        "    \n",
        "    def detect_objects(self, image, text_prompt, num_regions=3):\n",
        "        \"\"\"Detect regions using CLIP-based sliding window\"\"\"\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image.astype(np.uint8))\n",
        "        \n",
        "        h, w = image.size[1], image.size[0]\n",
        "        \n",
        "        # Create multiple search regions\n",
        "        regions = [\n",
        "            [w*0.1, h*0.1, w*0.6, h*0.6],  # Top-left\n",
        "            [w*0.2, h*0.2, w*0.8, h*0.8],  # Center\n",
        "            [w*0.4, h*0.4, w*0.9, h*0.9],  # Bottom-right\n",
        "            [w*0.0, h*0.0, w*1.0, h*1.0],  # Full image\n",
        "        ]\n",
        "        \n",
        "        best_boxes = []\n",
        "        best_scores = []\n",
        "        \n",
        "        for region in regions[:num_regions]:\n",
        "            x1, y1, x2, y2 = [int(x) for x in region]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w, x2), min(h, y2)\n",
        "            \n",
        "            if x2 > x1 and y2 > y1:\n",
        "                crop = image.crop((x1, y1, x2, y2))\n",
        "                score = self.clip_scorer.score(crop, text_prompt)\n",
        "                \n",
        "                if score > 0.15:  # Threshold for detection\n",
        "                    best_boxes.append([x1, y1, x2, y2])\n",
        "                    best_scores.append(score)\n",
        "        \n",
        "        if not best_boxes:\n",
        "            # Fallback: use center region\n",
        "            cx, cy = w//2, h//2\n",
        "            size = min(w, h) // 3\n",
        "            best_boxes = [[cx-size, cy-size, cx+size, cy+size]]\n",
        "            best_scores = [0.1]\n",
        "        \n",
        "        return np.array(best_boxes), np.array(best_scores)\n",
        "\n",
        "# Create hybrid detector that tries GroundingDINO first, then falls back to CLIP\n",
        "class HybridDetector:\n",
        "    \"\"\"Hybrid detector: GroundingDINO + CLIP fallback for maximum accuracy\"\"\"\n",
        "    \n",
        "    def __init__(self, clip_scorer):\n",
        "        self.grounding_dino = GroundingDINODetector()\n",
        "        self.clip_detector = CLIPBasedDetector(clip_scorer)\n",
        "    \n",
        "    def detect_objects(self, image, text_prompt, num_regions=3):\n",
        "        \"\"\"Try GroundingDINO first, fallback to CLIP\"\"\"\n",
        "        # Try GroundingDINO first\n",
        "        boxes, scores = self.grounding_dino.detect_objects(image, text_prompt)\n",
        "        \n",
        "        if len(boxes) > 0:\n",
        "            print(f\"   ‚úÖ GroundingDINO found {len(boxes)} regions\")\n",
        "            return boxes, scores\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  GroundingDINO failed, using CLIP fallback\")\n",
        "            return self.clip_detector.detect_objects(image, text_prompt, num_regions)\n",
        "\n",
        "# Initialize hybrid detector\n",
        "detector = HybridDetector(clip_scorer)\n",
        "print(\"‚úÖ Hybrid detector (GroundingDINO + CLIP) ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 6. COMPREHENSIVE MASK EVALUATOR\n",
        "# ============================================================================\n",
        "\n",
        "class ComprehensiveMaskEvaluator:\n",
        "    \"\"\"Evaluate masks using multiple quality criteria\"\"\"\n",
        "    \n",
        "    def __init__(self, clip_scorer):\n",
        "        self.clip_scorer = clip_scorer\n",
        "    \n",
        "    def evaluate_mask(self, image, mask, text_prompt):\n",
        "        \"\"\"Comprehensive mask evaluation\"\"\"\n",
        "        mask = np.asarray(mask).astype(bool)\n",
        "        \n",
        "        # 1. Text similarity (most important)\n",
        "        text_score = self._compute_text_score(image, mask, text_prompt)\n",
        "        \n",
        "        # 2. Mask quality metrics\n",
        "        compactness = self._compute_compactness(mask)\n",
        "        size_score = self._compute_size_score(mask)\n",
        "        smoothness = self._compute_smoothness(mask)\n",
        "        \n",
        "        # 3. Combined score (ENHANCED weights for higher accuracy)\n",
        "        total_score = (\n",
        "            0.6 * text_score +      # Higher weight for text similarity\n",
        "            0.2 * compactness +     # Shape quality\n",
        "            0.1 * size_score +      # Size appropriateness\n",
        "            0.1 * smoothness        # Boundary quality\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'total': total_score,\n",
        "            'text': text_score,\n",
        "            'compactness': compactness,\n",
        "            'size': size_score,\n",
        "            'smoothness': smoothness\n",
        "        }\n",
        "    \n",
        "    def _compute_text_score(self, image, mask, text_prompt, padding=20):\n",
        "        \"\"\"Compute CLIP similarity for masked region\"\"\"\n",
        "        coords = np.argwhere(mask)\n",
        "        if len(coords) == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        y1, x1 = coords.min(axis=0)\n",
        "        y2, x2 = coords.max(axis=0)\n",
        "        h, w = image.shape[:2]\n",
        "        \n",
        "        # Add padding\n",
        "        y1 = max(0, y1 - padding)\n",
        "        x1 = max(0, x1 - padding)\n",
        "        y2 = min(h, y2 + padding)\n",
        "        x2 = min(w, x2 + padding)\n",
        "        \n",
        "        if y2 <= y1 or x2 <= x1:\n",
        "            return 0.0\n",
        "        \n",
        "        crop = image[y1:y2, x1:x2]\n",
        "        \n",
        "        try:\n",
        "            score = self.clip_scorer.score(crop, text_prompt)\n",
        "            # ENHANCED normalization for higher scores (matching your original 0.805)\n",
        "            return max(0, min(1, (score + 0.2) / 0.6))\n",
        "        except:\n",
        "            return 0.0\n",
        "    \n",
        "    def _compute_compactness(self, mask):\n",
        "        \"\"\"Compute mask compactness\"\"\"\n",
        "        area = mask.sum()\n",
        "        if area == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if not contours:\n",
        "            return 0.0\n",
        "        \n",
        "        contour = max(contours, key=cv2.contourArea)\n",
        "        perimeter = cv2.arcLength(contour, True)\n",
        "        \n",
        "        compactness = (perimeter * perimeter) / area\n",
        "        return max(0, 1 - compactness / 100)\n",
        "    \n",
        "    def _compute_size_score(self, mask):\n",
        "        \"\"\"Compute size appropriateness\"\"\"\n",
        "        ratio = mask.sum() / mask.size\n",
        "        if 0.05 <= ratio <= 0.5:\n",
        "            return 1.0\n",
        "        elif ratio < 0.05:\n",
        "            return ratio / 0.05\n",
        "        else:\n",
        "            return max(0, 1 - (ratio - 0.5) / 0.5)\n",
        "    \n",
        "    def _compute_smoothness(self, mask):\n",
        "        \"\"\"Compute boundary smoothness\"\"\"\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "        smoothed = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
        "        smoothed = cv2.morphologyEx(smoothed, cv2.MORPH_OPEN, kernel)\n",
        "        \n",
        "        iou = np.logical_and(mask, smoothed).sum() / np.logical_or(mask, smoothed).sum()\n",
        "        return iou\n",
        "    \n",
        "    def select_best_mask(self, image, masks, text_prompt):\n",
        "        \"\"\"Select best mask from candidates\"\"\"\n",
        "        best_mask = None\n",
        "        best_score = -1\n",
        "        best_details = None\n",
        "        \n",
        "        for mask in masks:\n",
        "            details = self.evaluate_mask(image, mask, text_prompt)\n",
        "            if details['total'] > best_score:\n",
        "                best_score = details['total']\n",
        "                best_mask = mask\n",
        "                best_details = details\n",
        "        \n",
        "        return best_mask, best_details\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ComprehensiveMaskEvaluator(clip_scorer)\n",
        "print(\"‚úÖ Comprehensive mask evaluator ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 7. MAIN SEGMENTATION PIPELINE (CLEAN & OPTIMIZED)\n",
        "# ============================================================================\n",
        "\n",
        "class TextDrivenSegmentationPipeline:\n",
        "    \"\"\"Clean, optimized pipeline for text-driven segmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, detector, sam2_predictor, evaluator, output_dir):\n",
        "        self.detector = detector\n",
        "        self.sam2_predictor = sam2_predictor\n",
        "        self.evaluator = evaluator\n",
        "        self.output_dir = output_dir\n",
        "    \n",
        "    def segment_image(self, image, text_prompt, save_results=True, filename_prefix=\"result\"):\n",
        "        \"\"\"Complete segmentation pipeline\"\"\"\n",
        "        print(f\"\\nüéØ TEXT-DRIVEN SEGMENTATION\")\n",
        "        print(f\"üìù Prompt: '{text_prompt}'\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Prepare image\n",
        "        if isinstance(image, Image.Image):\n",
        "            image_pil = image\n",
        "            image = np.array(image)\n",
        "        else:\n",
        "            image_pil = Image.fromarray(image.astype(np.uint8))\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        # Step 1: Detect regions\n",
        "        print(\"üîç Step 1: Detecting regions...\")\n",
        "        boxes, detection_scores = self.detector.detect_objects(image, text_prompt)\n",
        "        print(f\"   Found {len(boxes)} regions\")\n",
        "        \n",
        "        # Step 2: SAM 2 segmentation\n",
        "        print(\"üé® Step 2: Generating masks...\")\n",
        "        all_masks, all_scores = self._generate_masks(image, boxes)\n",
        "        print(f\"   Generated {len(all_masks)} mask candidates\")\n",
        "        \n",
        "        # Step 3: Evaluate and select best mask\n",
        "        print(\"‚öñÔ∏è  Step 3: Evaluating masks...\")\n",
        "        best_mask, quality_scores = self.evaluator.select_best_mask(image, all_masks, text_prompt)\n",
        "        print(f\"   Best mask quality: {quality_scores['total']:.3f}\")\n",
        "        \n",
        "        # Step 4: Post-process\n",
        "        print(\"üîß Step 4: Post-processing...\")\n",
        "        final_mask = self._post_process_mask(best_mask)\n",
        "        \n",
        "        # Store results\n",
        "        results = {\n",
        "            'boxes': boxes,\n",
        "            'detection_scores': detection_scores,\n",
        "            'all_masks': all_masks,\n",
        "            'all_scores': all_scores,\n",
        "            'best_mask': best_mask,\n",
        "            'final_mask': final_mask,\n",
        "            'quality_scores': quality_scores\n",
        "        }\n",
        "        \n",
        "        # Save and visualize\n",
        "        if save_results:\n",
        "            self._save_and_visualize(image, results, filename_prefix)\n",
        "        \n",
        "        print(f\"\\n‚úÖ SEGMENTATION COMPLETE!\")\n",
        "        print(f\"üìä Final Score: {quality_scores['total']:.3f}\")\n",
        "        print(f\"üìè Mask Coverage: {final_mask.sum() / final_mask.size * 100:.1f}%\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _generate_masks(self, image, boxes):\n",
        "        \"\"\"Generate masks using SAM 2\"\"\"\n",
        "        self.sam2_predictor.set_image(image)\n",
        "        \n",
        "        all_masks = []\n",
        "        all_scores = []\n",
        "        \n",
        "        for box in boxes:\n",
        "            try:\n",
        "                masks, scores, _ = self.sam2_predictor.predict(\n",
        "                    point_coords=None,\n",
        "                    point_labels=None,\n",
        "                    box=box[None, :],\n",
        "                    multimask_output=True\n",
        "                )\n",
        "                \n",
        "                for mask, score in zip(masks, scores):\n",
        "                    all_masks.append(mask)\n",
        "                    all_scores.append(score)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        if not all_masks:\n",
        "            # Fallback\n",
        "            h, w = image.shape[:2]\n",
        "            fallback = np.zeros((h, w), dtype=bool)\n",
        "            fallback[h//4:3*h//4, w//4:3*w//4] = True\n",
        "            all_masks = [fallback]\n",
        "            all_scores = [0.1]\n",
        "        \n",
        "        return all_masks, np.array(all_scores)\n",
        "    \n",
        "    def _post_process_mask(self, mask):\n",
        "        \"\"\"Clean up the mask\"\"\"\n",
        "        mask = np.asarray(mask).astype(bool)\n",
        "        \n",
        "        # Remove small holes and islands\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "        mask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "        \n",
        "        # Smooth boundary\n",
        "        mask = cv2.GaussianBlur(mask.astype(np.float32), (5, 5), 1)\n",
        "        \n",
        "        return (mask > 0.5).astype(bool)\n",
        "    \n",
        "    def _save_and_visualize(self, image, results, filename_prefix):\n",
        "        \"\"\"Save results and create visualization - FIXED VERSION\"\"\"\n",
        "        # Save files\n",
        "        Image.fromarray(image).save(os.path.join(self.output_dir, f\"{filename_prefix}_original.png\"))\n",
        "        \n",
        "        final_mask = results['final_mask']\n",
        "        mask_img = Image.fromarray((final_mask * 255).astype(np.uint8))\n",
        "        mask_img.save(os.path.join(self.output_dir, f\"{filename_prefix}_mask.png\"))\n",
        "        \n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        \n",
        "        # Top row\n",
        "        axes[0,0].imshow(image)\n",
        "        axes[0,0].set_title(\"Original Image\")\n",
        "        axes[0,0].axis('off')\n",
        "        \n",
        "        axes[0,1].imshow(final_mask, cmap='gray')\n",
        "        axes[0,1].set_title(\"Segmentation Mask\")\n",
        "        axes[0,1].axis('off')\n",
        "        \n",
        "        # FIXED: Ensure final_mask is boolean\n",
        "        final_mask_bool = np.asarray(final_mask).astype(bool)\n",
        "        overlay = image.copy()\n",
        "        overlay[final_mask_bool] = [255, 0, 0]\n",
        "        axes[0,2].imshow(overlay)\n",
        "        axes[0,2].set_title(\"Final Result\")\n",
        "        axes[0,2].axis('off')\n",
        "        \n",
        "        # Bottom row - mask candidates\n",
        "        for i in range(min(3, len(results['all_masks']))):\n",
        "            mask = results['all_masks'][i]\n",
        "            # FIXED: Ensure mask is boolean\n",
        "            mask_bool = np.asarray(mask).astype(bool)\n",
        "            overlay = image.copy()\n",
        "            overlay[mask_bool] = [255, 0, 0]\n",
        "            axes[1,i].imshow(overlay)\n",
        "            score = results['all_scores'][i] if i < len(results['all_scores']) else 0\n",
        "            axes[1,i].set_title(f\"Candidate {i+1} (Score: {score:.3f})\")\n",
        "            axes[1,i].axis('off')\n",
        "        \n",
        "        plt.suptitle(f'Text-Driven Segmentation Results\\\\nPrompt: \"{results.get(\"text_prompt\", \"N/A\")}\"', \n",
        "                     fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f\"{filename_prefix}_results.png\"), \n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"üíæ Results saved to: {self.output_dir}\")\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = TextDrivenSegmentationPipeline(detector, sam2_predictor, evaluator, output_dir)\n",
        "print(\"‚úÖ Complete segmentation pipeline ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 8. EASY-TO-USE FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def segment_image_from_path(image_path, text_prompt, filename_prefix=None):\n",
        "    \"\"\"\n",
        "    üöÄ Easy-to-use function for segmenting images\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to image (local file or URL)\n",
        "        text_prompt: Text description of what to segment\n",
        "        filename_prefix: Optional prefix for saved files\n",
        "    \n",
        "    Returns:\n",
        "        results: Dictionary with segmentation results\n",
        "    \"\"\"\n",
        "    print(f\"üñºÔ∏è  Loading image: {image_path}\")\n",
        "    \n",
        "    # Load image\n",
        "    try:\n",
        "        if image_path.startswith('http'):\n",
        "            response = requests.get(image_path)\n",
        "            image = Image.open(BytesIO(response.content))\n",
        "        else:\n",
        "            image = Image.open(image_path)\n",
        "        \n",
        "        image = image.convert('RGB')\n",
        "        \n",
        "        # Resize if too large\n",
        "        if max(image.size) > 1024:\n",
        "            ratio = 1024 / max(image.size)\n",
        "            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
        "            image = image.resize(new_size, Image.LANCZOS)\n",
        "            print(f\"üìè Resized to: {new_size}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading image: {e}\")\n",
        "        return None\n",
        "    \n",
        "    # Generate filename\n",
        "    if filename_prefix is None:\n",
        "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "        filename_prefix = f\"{base_name}_segmented\"\n",
        "    \n",
        "    # Run segmentation\n",
        "    results = pipeline.segment_image(image, text_prompt, save_results=True, filename_prefix=filename_prefix)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def quick_segment(image_path, text_prompt):\n",
        "    \"\"\"\n",
        "    ‚ö° Quick segmentation with minimal output\n",
        "    \"\"\"\n",
        "    results = segment_image_from_path(image_path, text_prompt)\n",
        "    \n",
        "    if results:\n",
        "        score = results['quality_scores']['total']\n",
        "        coverage = results['final_mask'].sum() / results['final_mask'].size * 100\n",
        "        print(f\"\\nüìä Quick Results:\")\n",
        "        print(f\"   Score: {score:.3f}\")\n",
        "        print(f\"   Coverage: {coverage:.1f}%\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Easy-to-use functions ready!\")\n",
        "print(\"\\nüìñ Usage Examples:\")\n",
        "print(\"   # Full segmentation:\")\n",
        "print(\"   results = segment_image_from_path('/path/to/image.jpg', 'a red car')\")\n",
        "print(\"   \")\n",
        "print(\"   # Quick segmentation:\")\n",
        "print(\"   quick_segment('/path/to/image.jpg', 'a blue dog')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 9. DEMO - TEST THE PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "# Download a sample image\n",
        "def download_sample_image():\n",
        "    \"\"\"Download a sample image for testing\"\"\"\n",
        "    url = \"https://images.unsplash.com/photo-1543466835-00a7907e9de1?w=800\"\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    return image\n",
        "\n",
        "print(\"üì• Downloading sample image...\")\n",
        "try:\n",
        "    demo_image = download_sample_image()\n",
        "    print(\"‚úÖ Sample image downloaded!\")\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(demo_image)\n",
        "    plt.title(\"Sample Image for Testing\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Could not download image: {e}\")\n",
        "    print(\"Please upload your own image or use a local file\")\n",
        "\n",
        "print(\"‚úÖ Demo setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 10. RUN DEMO SEGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "# Test the pipeline with the sample image\n",
        "text_prompt = \"a golden brown dog sitting on grass\"\n",
        "\n",
        "print(\"üéØ RUNNING DEMO SEGMENTATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Run segmentation\n",
        "results = pipeline.segment_image(\n",
        "    image=demo_image,\n",
        "    text_prompt=text_prompt,\n",
        "    save_results=True,\n",
        "    filename_prefix=\"demo_dog\"\n",
        ")\n",
        "\n",
        "print(\"\\nüéâ DEMO COMPLETE!\")\n",
        "print(\"\\nüìã Summary:\")\n",
        "print(f\"   üìù Prompt: '{text_prompt}'\")\n",
        "print(f\"   üìä Final Score: {results['quality_scores']['total']:.3f}\")\n",
        "print(f\"   üé® Text Similarity: {results['quality_scores']['text']:.3f}\")\n",
        "print(f\"   üìè Mask Coverage: {results['final_mask'].sum() / results['final_mask'].size * 100:.1f}%\")\n",
        "print(f\"   üíæ Results saved to: {output_dir}\")\n",
        "\n",
        "print(\"\\nüöÄ Pipeline is ready for your own images!\")\n",
        "print(\"\\nüí° Try these examples:\")\n",
        "print(\"   segment_image_from_path('/path/to/your/image.jpg', 'a red car')\")\n",
        "print(\"   quick_segment('https://example.com/image.jpg', 'a blue bird')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# ============================================================================\n",
        "# For video segmentation\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MINIMAL VIDEO SEGMENTATION WITH SAM 2 - CLEAN & RELIABLE\n",
        "# ============================================================================\n",
        "# Minimal implementation for segmenting cars in traffic videos\n",
        "# Optimized for T4 GPU with robust error handling\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ Starting Minimal Video Segmentation Pipeline\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1. INSTALLATION & SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Install minimal dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q opencv-python-headless scipy scikit-image\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\n",
        "!pip install -q timm einops\n",
        "!pip install -q open_clip_torch\n",
        "!pip install -q matplotlib pillow numpy tqdm\n",
        "\n",
        "# Mount Google Drive and setup paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "output_dir = \"/content/drive/MyDrive/internship_oct/Q2_updated\"\n",
        "video_path = \"/content/drive/MyDrive/internship_oct/Q2_updated/1.mp4\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"üìÅ Video: {video_path}\")\n",
        "print(f\"üìÅ Output: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 2. IMPORTS & DEVICE SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "import sys\n",
        "\n",
        "# CLIP for text-image similarity\n",
        "import open_clip\n",
        "\n",
        "# SAM 2 imports\n",
        "if not os.path.exists('segment-anything-2'):\n",
        "    !git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "sys.path.append('./segment-anything-2')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üîß Device: {device}\")\n",
        "print(f\"üîß PyTorch: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üîß GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 3. SIMPLE CLIP SCORER (OPTIMIZED FOR T4 GPU)\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleCLIPScorer:\n",
        "    \"\"\"Simple CLIP scorer optimized for T4 GPU\"\"\"\n",
        "    \n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.preprocess = None\n",
        "        self.tokenizer = None\n",
        "        self.load_model()\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Load single best CLIP model for efficiency\"\"\"\n",
        "        print(\"üì• Loading CLIP model (optimized for T4 GPU)...\")\n",
        "        \n",
        "        try:\n",
        "            # Use ViT-L-14 which gives good accuracy without being too heavy for T4\n",
        "            self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
        "                'ViT-L-14', pretrained='openai', device=self.device\n",
        "            )\n",
        "            self.tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
        "            self.model.eval()\n",
        "            print(\"  ‚úÖ ViT-L-14 loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Failed to load ViT-L-14: {e}\")\n",
        "            # Fallback to smaller model\n",
        "            self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
        "                'ViT-B-32', pretrained='openai', device=self.device\n",
        "            )\n",
        "            self.tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "            self.model.eval()\n",
        "            print(\"  ‚úÖ ViT-B-32 loaded as fallback\")\n",
        "    \n",
        "    def score(self, image, text):\n",
        "        \"\"\"Score image-text similarity\"\"\"\n",
        "        try:\n",
        "            if isinstance(image, np.ndarray):\n",
        "                image = Image.fromarray(image.astype(np.uint8))\n",
        "            \n",
        "            if isinstance(text, str):\n",
        "                text = [text]\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "                text_input = self.tokenizer(text).to(self.device)\n",
        "                \n",
        "                image_features = self.model.encode_image(image_input)\n",
        "                text_features = self.model.encode_text(text_input)\n",
        "                \n",
        "                image_features = F.normalize(image_features, dim=-1)\n",
        "                text_features = F.normalize(text_features, dim=-1)\n",
        "                \n",
        "                similarity = (image_features @ text_features.T).squeeze()\n",
        "                return similarity.cpu().item()\n",
        "        except Exception as e:\n",
        "            print(f\"CLIP scoring error: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "# Initialize CLIP scorer\n",
        "clip_scorer = SimpleCLIPScorer(device=device)\n",
        "print(\"‚úÖ CLIP scorer ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 4. SAM 2 SETUP (OPTIMIZED FOR T4 GPU)\n",
        "# ============================================================================\n",
        "\n",
        "# Download SAM 2 checkpoint\n",
        "os.makedirs('./checkpoints', exist_ok=True)\n",
        "sam2_checkpoint = \"./checkpoints/sam2_hiera_base_plus.pt\"\n",
        "model_cfg = \"sam2_hiera_b+.yaml\"\n",
        "\n",
        "if not os.path.exists(sam2_checkpoint):\n",
        "    print(\"üì• Downloading SAM 2 checkpoint...\")\n",
        "    url = \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\"\n",
        "    urllib.request.urlretrieve(url, sam2_checkpoint)\n",
        "    print(\"‚úÖ SAM 2 checkpoint downloaded!\")\n",
        "\n",
        "# Import and setup SAM 2\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "print(\"üîß Building SAM 2 predictor...\")\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
        "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
        "print(\"‚úÖ SAM 2 ready for inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 5. SIMPLE CAR DETECTOR\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleCarDetector:\n",
        "\n",
        "    \n",
        "    def __init__(self, clip_scorer):\n",
        "        self.clip_scorer = clip_scorer\n",
        "    \n",
        "    def detect_cars(self, image, text_prompt=\"\"):\n",
        "\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image.astype(np.uint8))\n",
        "        \n",
        "        h, w = image.size[1], image.size[0]\n",
        "        \n",
        "        # Create search regions for cars (typically in lower half of image)\n",
        "        regions = [\n",
        "            [w*0.0, h*0.3, w*0.5, h*0.9],   # Left side\n",
        "            [w*0.5, h*0.3, w*1.0, h*0.9],   # Right side\n",
        "            [w*0.2, h*0.4, w*0.8, h*0.8],   # Center\n",
        "            [w*0.0, h*0.0, w*1.0, h*1.0],   # Full image\n",
        "        ]\n",
        "        \n",
        "        best_boxes = []\n",
        "        best_scores = []\n",
        "        \n",
        "        for region in regions:\n",
        "            x1, y1, x2, y2 = [int(x) for x in region]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w, x2), min(h, y2)\n",
        "            \n",
        "            if x2 > x1 and y2 > y1:\n",
        "                crop = image.crop((x1, y1, x2, y2))\n",
        "                score = self.clip_scorer.score(crop, text_prompt)\n",
        "                \n",
        "                if score > 0.2:  # Threshold for car detection\n",
        "                    best_boxes.append([x1, y1, x2, y2])\n",
        "                    best_scores.append(score)\n",
        "        \n",
        "        if not best_boxes:\n",
        "            # Fallback: use center region\n",
        "            cx, cy = w//2, h//2\n",
        "            size = min(w, h) // 4\n",
        "            best_boxes = [[cx-size, cy-size, cx+size, cy+size]]\n",
        "            best_scores = [0.5]\n",
        "        \n",
        "        return np.array(best_boxes), np.array(best_scores)\n",
        "\n",
        "# Initialize detector\n",
        "detector = SimpleCarDetector(clip_scorer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 6. MINIMAL VIDEO SEGMENTATION PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "class MinimalVideoSegmenter:\n",
        "    \"\"\"Minimal video segmentation pipeline optimized for T4 GPU\"\"\"\n",
        "    \n",
        "    def __init__(self, detector, sam2_predictor, clip_scorer, output_dir):\n",
        "        self.detector = detector\n",
        "        self.sam2_predictor = sam2_predictor\n",
        "        self.clip_scorer = clip_scorer\n",
        "        self.output_dir = output_dir\n",
        "    \n",
        "    def segment_video(self, video_path, text_prompt=\"\", max_frames=150):\n",
        "        \"\"\"Segment object in video - minimal implementation\"\"\"\n",
        "        print(f\"\\nüé¨ SEGMENTING OBJECT IN VIDEO\")\n",
        "        print(f\"üìù Prompt: '{text_prompt}'\")\n",
        "        print(f\"üé• Video: {video_path}\")\n",
        "        print(f\"üéûÔ∏è  Max frames: {max_frames}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        try:\n",
        "            # Step 1: Load video\n",
        "            print(\"\\nüìπ Loading video...\")\n",
        "            frames = self._load_video_frames(video_path, max_frames)\n",
        "            print(f\"   ‚úÖ Loaded {len(frames)} frames\")\n",
        "            \n",
        "            # Step 2: Detect object in first frame\n",
        "            print(\"\\nüîç Detecting Object in first frame...\")\n",
        "            first_frame = frames[0]\n",
        "            boxes, scores = self.detector.detect_cars(first_frame, text_prompt)\n",
        "            \n",
        "            # Select best detection\n",
        "            best_idx = np.argmax(scores)\n",
        "            target_box = boxes[best_idx]\n",
        "            print(f\"   ‚úÖ Best detection: score={scores[best_idx]:.3f}\")\n",
        "            print(f\"   üìç Box: {target_box}\")\n",
        "            \n",
        "            # Step 3: Segment each frame\n",
        "            print(\"\\nüé® Segmenting frames...\")\n",
        "            masks = self._segment_frames(frames, target_box)\n",
        "            print(f\"   ‚úÖ Generated {len(masks)} masks\")\n",
        "            \n",
        "            # Step 4: Create output video\n",
        "            print(\"\\nüíæ Creating output video...\")\n",
        "            output_path = self._create_output_video(frames, masks, text_prompt)\n",
        "            \n",
        "            print(f\"\\n‚úÖ VIDEO SEGMENTATION COMPLETE!\")\n",
        "            print(f\"üé• Output: {output_path}\")\n",
        "            \n",
        "            return {\n",
        "                'frames': frames,\n",
        "                'masks': masks,\n",
        "                'output_video': output_path,\n",
        "                'target_box': target_box\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Segmentation failed: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def _load_video_frames(self, video_path, max_frames):\n",
        "        \"\"\"Load video frames\"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        \n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Could not open video: {video_path}\")\n",
        "        \n",
        "        frames = []\n",
        "        frame_count = 0\n",
        "        \n",
        "        while frame_count < max_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            # Convert BGR to RGB and resize for efficiency\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # Resize if too large (optimize for T4 GPU)\n",
        "            h, w = frame_rgb.shape[:2]\n",
        "            if max(h, w) > 1024:\n",
        "                ratio = 1024 / max(h, w)\n",
        "                new_h, new_w = int(h * ratio), int(w * ratio)\n",
        "                frame_rgb = cv2.resize(frame_rgb, (new_w, new_h))\n",
        "            \n",
        "            frames.append(frame_rgb)\n",
        "            frame_count += 1\n",
        "        \n",
        "        cap.release()\n",
        "        \n",
        "        if len(frames) == 0:\n",
        "            raise ValueError(f\"No frames could be extracted from video\")\n",
        "        \n",
        "        return frames\n",
        "    \n",
        "    def _segment_frames(self, frames, target_box):\n",
        "        \"\"\"Segment each frame using SAM 2\"\"\"\n",
        "        masks = []\n",
        "        \n",
        "        for i, frame in enumerate(tqdm(frames, desc=\"Segmenting\")):\n",
        "            try:\n",
        "                # Set image for SAM 2\n",
        "                self.sam2_predictor.set_image(frame)\n",
        "                \n",
        "                # Generate masks\n",
        "                frame_masks, frame_scores, _ = self.sam2_predictor.predict(\n",
        "                    point_coords=None,\n",
        "                    point_labels=None,\n",
        "                    box=target_box[None, :],\n",
        "                    multimask_output=True\n",
        "                )\n",
        "                \n",
        "                # Select best mask\n",
        "                best_mask_idx = np.argmax(frame_scores)\n",
        "                mask = frame_masks[best_mask_idx]\n",
        "                \n",
        "                # Clean up mask\n",
        "                mask = self._clean_mask(mask)\n",
        "                masks.append(mask)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è  Frame {i} failed: {e}\")\n",
        "                # Use previous mask or empty mask\n",
        "                if masks:\n",
        "                    masks.append(masks[-1].copy())\n",
        "                else:\n",
        "                    h, w = frame.shape[:2]\n",
        "                    masks.append(np.zeros((h, w), dtype=bool))\n",
        "        \n",
        "        return masks\n",
        "    \n",
        "    def _clean_mask(self, mask):\n",
        "        \"\"\"Clean up mask\"\"\"\n",
        "        # Remove noise\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "        mask_uint8 = mask.astype(np.uint8)\n",
        "        mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_CLOSE, kernel)\n",
        "        mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_OPEN, kernel)\n",
        "        \n",
        "        # Smooth\n",
        "        mask_float = cv2.GaussianBlur(mask_uint8.astype(np.float32), (3, 3), 1)\n",
        "        \n",
        "        return (mask_float > 0.5).astype(bool)\n",
        "    \n",
        "    def _create_output_video(self, frames, masks, text_prompt):\n",
        "        \"\"\"Create output video with segmentation overlay\"\"\"\n",
        "        output_path = os.path.join(self.output_dir, \"cars_segmented.mp4\")\n",
        "        \n",
        "        h, w = frames[0].shape[:2]\n",
        "        fps = 15  # Reduced FPS for efficiency\n",
        "        \n",
        "        # Create video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out_video = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "        \n",
        "        # Process each frame\n",
        "        for frame, mask in tqdm(zip(frames, masks), total=len(frames), desc=\"Creating video\"):\n",
        "            # Create overlay\n",
        "            overlay = frame.copy()\n",
        "            mask_bool = np.asarray(mask).astype(bool)\n",
        "            \n",
        "            # Red overlay for cars\n",
        "            overlay[mask_bool] = overlay[mask_bool] * 0.6 + np.array([255, 0, 0]) * 0.4\n",
        "            \n",
        "            # Add contour\n",
        "            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(overlay, contours, -1, (255, 255, 0), 2)\n",
        "            \n",
        "            # Write frame (convert RGB to BGR)\n",
        "            out_video.write(cv2.cvtColor(overlay.astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
        "        \n",
        "        out_video.release()\n",
        "        \n",
        "        return output_path\n",
        "\n",
        "# Initialize segmenter\n",
        "segmenter = MinimalVideoSegmenter(detector, sam2_predictor, clip_scorer, output_dir)\n",
        "print(\"‚úÖ Video segmenter ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 7. RUN OBJECT SEGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Check if video exists\n",
        "if os.path.exists(video_path):\n",
        "    print(f\"‚úÖ Video found: {video_path}\")\n",
        "    \n",
        "    # Run segmentation\n",
        "    results = segmenter.segment_video(\n",
        "        video_path=video_path,\n",
        "        text_prompt=\"cars\",\n",
        "        max_frames=150  # Process ~10 seconds at 15fps\n",
        "    )\n",
        "    \n",
        "    if results:\n",
        "        print(\"\\nüéâ SUCCESS!\")\n",
        "        print(f\"üéûÔ∏è  Frames processed: {len(results['frames'])}\")\n",
        "        print(f\"üé• Output video: {results['output_video']}\")\n",
        "        \n",
        "        # Show sample frames\n",
        "        print(\"\\nüìä Sample frames:\")\n",
        "        sample_indices = [0, len(results['frames'])//4, len(results['frames'])//2, -1]\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "        \n",
        "        for i, idx in enumerate(sample_indices):\n",
        "            frame = results['frames'][idx]\n",
        "            mask = results['masks'][idx]\n",
        "            \n",
        "            # Original frame\n",
        "            axes[0, i].imshow(frame)\n",
        "            axes[0, i].set_title(f\"Frame {idx}\")\n",
        "            axes[0, i].axis('off')\n",
        "            \n",
        "            # Overlay\n",
        "            overlay = frame.copy()\n",
        "            mask_bool = np.asarray(mask).astype(bool)\n",
        "            overlay[mask_bool] = overlay[mask_bool] * 0.6 + np.array([255, 0, 0]) * 0.4\n",
        "            axes[1, i].imshow(overlay)\n",
        "            axes[1, i].set_title(f\"Segmented {idx}\")\n",
        "            axes[1, i].axis('off')\n",
        "        \n",
        "        plt.suptitle('Car Segmentation Results', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    else:\n",
        "        print(\"\\n‚ùå Segmentation failed!\")\n",
        "        \n",
        "else:\n",
        "    print(f\"‚ùå Video not found: {video_path}\")\n",
        "    print(\"Please check the path and try again.\")\n",
        "\n",
        "print(\"\\n‚úÖ Done!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
